{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:p35]",
      "language": "python",
      "name": "conda-env-p35-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "nb-morpho-finalv5",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S44UD_Eg2u5c",
        "collapsed": true
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPaiGZ5YP6py"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Training Files/merged training data.zip' -d '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Mw2kweRI45"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Alankrita/Morphology task/Saved Models/model_CV.zip' -d '/content/models'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzfKiXj8Q5dV"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "4wCsBTP8D-1x"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from random import shuffle\n",
        "import cv2\n",
        "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
        "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
        "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
        "from keras.applications.densenet import DenseNet121, preprocess_input\n",
        "from keras.layers.pooling import _GlobalPooling1D\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.nasnet import NASNetMobile, NASNetLarge\n",
        "from glob import glob\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        " \n",
        "# The GPU id to use, usually either \"0\" or \"1\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atr_eFcED-2O"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o59tkXFD-2w"
      },
      "source": [
        "def path(row):\n",
        "    \n",
        "    return '/content/train/' + row.id "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uzJHNnUY56A"
      },
      "source": [
        "def get_id(file_path):\n",
        "    return file_path.split(os.path.sep)[-1].replace('.tif', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Yh2ojqD-3m"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/Merged Train labels/train_labels.csv\")\n",
        "id_label_map = {k:v for k,v in zip(train.id.values, train.label.values)}\n",
        "train.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTWOdRBD-4k"
      },
      "source": [
        "train[\"path\"] = train.apply(path, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXBG5M0BD-44"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbkvZqFMD-5I"
      },
      "source": [
        "train['index'] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DofD7rXD_C_"
      },
      "source": [
        "stratkf = StratifiedKFold(n_splits=3, random_state=8, shuffle=True)\n",
        "\n",
        "i = 0\n",
        "\n",
        "for train_index, test_index in stratkf.split(train, train['label']):\n",
        "    train.loc[test_index, 'index'] = i\n",
        "    \n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oHrpizCD_L4"
      },
      "source": [
        "ind = 2\n",
        "\n",
        "len(train[(train['index'] == ind) & (train['label'] == 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0nE0T4oD_MM"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryr_ynfrD_QW"
      },
      "source": [
        "lab_files = glob('/content/train/*.png')\n",
        "print(\"Labeled files size :\", len(lab_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_GEK1EPS8K4"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_up5SeyaBIeB"
      },
      "source": [
        "def slicer(seq, size):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "def get_seq():\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "    sequential = iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "            iaa.Flipud(0.2), # vertically flip 20% of all images\n",
        "            sometimes(iaa.Affine(\n",
        "                #scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
        "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n",
        "                rotate=(-90, 90), # rotate by -45 to +45 degrees\n",
        "                shear=(-5, 5), # shear by -16 to +16 degrees\n",
        "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
        "                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per image\n",
        "            # don't execute all of them, as that would often be way too strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                [\n",
        "                    sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
        "                    iaa.OneOf([\n",
        "                        iaa.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n",
        "                        iaa.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n",
        "                        iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n",
        "                    ]),\n",
        "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), # sharpen images\n",
        "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
        "                    # search either for all edges or for directed edges,\n",
        "                    # blend the result with the original image using a blobby mask\n",
        "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
        "                    ])),\n",
        "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Dropout((0.01, 0.05), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
        "                        iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n",
        "                    ]),\n",
        "                    iaa.Invert(0.01, per_channel=True), # invert color channels\n",
        "                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
        "                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation\n",
        "                    # either change the brightness of the whole image (sometimes\n",
        "                    # per channel) or change the brightness of subareas\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                        iaa.FrequencyNoiseAlpha(\n",
        "                            exponent=(-1, 0),\n",
        "                            first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
        "                            second=iaa.ContrastNormalization((0.9, 1.1))\n",
        "                        )\n",
        "                    ]),\n",
        "                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
        "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n",
        "                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "                ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "    return sequential\n",
        "\n",
        "def data_gen(list_files, id_label_map, batch_size, augment=False):\n",
        "    sequential = get_seq()\n",
        "    while True:\n",
        "        shuffle(list_files)\n",
        "        for batch in slicer(list_files, batch_size):\n",
        "            X = [cv2.imread(x) for x in batch]\n",
        "            Y = [id_label_map[get_id(x)] for x in batch]\n",
        "            if augment:\n",
        "                X = sequential.augment_images(X)\n",
        "            X = [preprocess_input(x) for x in X]\n",
        "                \n",
        "            yield np.array(X), np.array(Y)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmjX130SyOwC"
      },
      "source": [
        "# Selective Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhkJBdegxZG8"
      },
      "source": [
        "for i in range(3):\n",
        "   train_position = train[(train['index'] != i) & (train['label'] == 1)]\n",
        "   train_only = train[train['index'] != i]\n",
        "   train_enhance = pd.concat([train_only,train_position,train_position,train_position,train_position,train_position, train_position, train_position], ignore_index=True)\n",
        "   traindata = train_enhance.loc[train_enhance['index'] != i , 'path']\n",
        "   val = train.loc[train['index'] == i , 'path']\n",
        "    \n",
        "   print(\"CV: \" + str(i))\n",
        "   print(\"Train: \" + str(len(traindata)))\n",
        "   print(\"Val: \" + str(len(val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0M8FIsvD_Qv"
      },
      "source": [
        "images = data_gen(train.loc[train['index'] == 0 , 'path'], id_label_map, batch_size=18, augment=True)\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "for (image, y) in images:\n",
        "    for i, _im in enumerate(image):\n",
        "        plt.subplot(3,6,i+1)\n",
        "        plt.imshow((_im-_im.min())/ (_im.max() - _im.min()))\n",
        "        plt.axis('off')\n",
        "    break\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxiRJBk5cZok"
      },
      "source": [
        "# Fitting and training the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97epu6m1D_RO"
      },
      "source": [
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "\n",
        "def model_resnet(): \n",
        "    inputs = Input((130, 130, 3))\n",
        "    b_model = ResNet50V2(include_top=False, input_shape=(130, 130, 3),weights='imagenet') #CHANGE\n",
        "    x = b_model(inputs)\n",
        "    op1 = GlobalMaxPooling2D()(x)\n",
        "    op2 = GlobalAveragePooling2D()(x)\n",
        "    op3 = Flatten()(x)\n",
        "    out = Concatenate(axis=-1)([op1, op2, op3])\n",
        "    out = Dropout(0.5)(out)\n",
        "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
        "    model = Model(inputs, out)\n",
        "    model.compile(optimizer=Adam(0.00001), loss=binary_crossentropy, metrics=['acc'])\n",
        "    print(\"Model compiled\")\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVhoUabA30ny"
      },
      "source": [
        "**Note:** **The model was trained using 10 epochs for 3 Cross Validation splits each, due to logistical insufficiency. However, it is recommended to the evaluators to uncomment all entire section and then train the model to get the original planned out results.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNgPOQrrxD6j"
      },
      "source": [
        "for i in range(3):\n",
        "\n",
        "    model = model_resnet()\n",
        "    \n",
        "    batch_size=64\n",
        "    h5_path = \"/content/modelz\" + str(i) + \".h5\"\n",
        "    checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "    history = model.fit_generator(\n",
        "        data_gen(traindata, id_label_map, batch_size, augment=True),\n",
        "        validation_data=data_gen(val, id_label_map, batch_size),\n",
        "        epochs=10, verbose=1,\n",
        "        callbacks=[checkpoint],\n",
        "        steps_per_epoch=len(traindata) // batch_size,\n",
        "        validation_steps=len(val) // batch_size)\n",
        "    batch_size=24\n",
        "    history = model.fit_generator(\n",
        "        data_gen(traindata, id_label_map, batch_size, augment=True),\n",
        "        validation_data=data_gen(val, id_label_map, batch_size),\n",
        "        epochs=50, verbose=1,\n",
        "        callbacks=[checkpoint],\n",
        "        steps_per_epoch=len(traindata) // batch_size,\n",
        "        validation_steps=len(val) // batch_size)\n",
        "    batch_size=32\n",
        "    history = model.fit_generator(\n",
        "        data_gen(traindata, id_label_map, batch_size, augment=True),\n",
        "        validation_data=data_gen(val, id_label_map, batch_size),\n",
        "        epochs=80, verbose=1,\n",
        "        callbacks=[checkpoint],\n",
        "        steps_per_epoch=len(traindata) // batch_size,\n",
        "        validation_steps=len(val) // batch_size)\n",
        "    del(model)\n",
        "    del(traindata)\n",
        "    del(val)\n",
        "    print(\"Model erased\")\n",
        "    \n",
        "\n",
        "#model.load_weights(h5_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2wkE1taD_SR"
      },
      "source": [
        " Now we predict each validation set on the respective model to find the best threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4wxYtmlD_ZY"
      },
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw4pyNR-YiDX"
      },
      "source": [
        "# Quality Score Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuy2Xv4stgU"
      },
      "source": [
        "!wget https://www.dropbox.com/s/4uwo0u0zbywbdpa/Sperm-Data.zip?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSXOaeqLCJkW"
      },
      "source": [
        "!unzip '/content/Sperm-Data.zip?dl=0' -d '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2OrbSylYJD3"
      },
      "source": [
        "test_folders = glob(\"/content/Sperm-Data/Sperm - Unlabeled/*/\")\n",
        "test_folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjpsFKGHXDHR"
      },
      "source": [
        "from glob import glob\n",
        "files = glob(\"/content/Sperm-Data/Sperm - Unlabeled/*/*/*/*\")\n",
        "files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt5w8ZnMD_cT"
      },
      "source": [
        "# Here we use one of the models to predict the unlabeled data and calculate the quality score.\n",
        "\n",
        "model.load_weights(h5_list[-3])\n",
        "\n",
        "for _test in test_folders:\n",
        "    preds = []\n",
        "    ids = []\n",
        "    files = glob(\"/content/Sperm-Data/Sperm - Unlabeled/*/*/*/*\")\n",
        "    for batch in slicer(files, batch_size):\n",
        "        \n",
        "        X = [preprocess_input(cv2.resize(cv2.imread(x), (130,130))) for x in batch]\n",
        "        ids_batch = [get_id(x) for x in batch]\n",
        "        X = np.array(X)\n",
        "        preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
        "        preds += preds_batch\n",
        "        ids += ids_batch\n",
        "    preds = np.array(preds)\n",
        "    preds = 1 * (preds > 0.29) # Make sure to change this threshold according to the value in the prior cells. Get the Thr number and divide by 1000 \n",
        "    print(_test.split(\"/\")[-2] + \": \" + str(int(100 * preds.sum()/len(preds)))) #formula to calculate\n",
        "    df = pd.DataFrame({'id':ids, 'label':preds})\n",
        "    df.to_csv(\"result_\" + _test.split(\"/\")[-2] + \".csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlar20VhC9yj"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCFOF16NTdHQ"
      },
      "source": [
        "# Model Speculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25u8d112D_Zi"
      },
      "source": [
        "\n",
        "\n",
        "model = get_model_classif_densenet()\n",
        "\n",
        "h5_list = sorted(glob(\"/content/models/*\"))# h5_list\n",
        "\n",
        "for i, h5_path in enumerate(tqdm(h5_list[-3:])):\n",
        "    model.load_weights(h5_path)\n",
        "    val = df_train.loc[df_train['index'] == i , 'path']\n",
        "    preds = []\n",
        "    ids = []\n",
        "    for batch in slicer(val, batch_size):\n",
        "        X = [preprocess_input(cv2.imread(x)) for x in batch]\n",
        "        ids_batch = [get_id_from_file_path(x) for x in batch]\n",
        "        X = np.array(X)\n",
        "        preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
        "        preds += preds_batch\n",
        "        ids += ids_batch\n",
        "    df = pd.DataFrame({'id':ids, 'label':preds})\n",
        "    df_merge = pd.merge(df, df_train, how='left', on='id')\n",
        "    df_merge[['label_x', 'label_y']].to_csv('res_crossval' + str(i) + '.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50UNu3yFD_bw"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyJqCvRmUPH1"
      },
      "source": [
        "F1-Scores - Validation Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNLK-wY8D_b9"
      },
      "source": [
        "best_th = []\n",
        "for i in range(3):\n",
        "    df = pd.read_csv('/content/res_crossval' + str(i) + '.csv')\n",
        "    y_test = df['label_y'].values\n",
        "    y_score = df['label_x'].values\n",
        "    f1 = []\n",
        "    for _t in range(0, 1001, 1):\n",
        "        th = _t / 1000\n",
        "        f1.append(f1_score(y_test, y_score > th, ))\n",
        "    f1 = np.array(f1)\n",
        "    print(\"F1 score: \" + str(f1[f1.argmax()]) + \" Threshold: \" + str(f1.argmax()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt91aSg9UD_8"
      },
      "source": [
        "df = pd.read_csv(\"/content/res_crossval0.csv\")\n",
        "df1 = pd.read_csv(\"/content/res_crossval1.csv\")\n",
        "df2 = pd.read_csv(\"/content/res_crossval2.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GAbi5WDUb8y"
      },
      "source": [
        "Test Acc_CV0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHO0xQAq5pqy"
      },
      "source": [
        "n_classes = 1\n",
        "y_test = df[\"label_y\"].values\n",
        "y_score = df[\"label_x\"].values\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "#Plot of a ROC curve for a specific class\n",
        "roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKBW6kaEUVgy"
      },
      "source": [
        "acc = []\n",
        "for _t in thr:\n",
        "    acc.append(accuracy_score(y_test, y_score > _t))\n",
        "\n",
        "acc = np.asarray(acc)\n",
        "acc[acc.argmax()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ho57cXlUg9w"
      },
      "source": [
        "Best Thr_CV0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL3VR2j9UeYh"
      },
      "source": [
        "best_thr = thr[acc.argmax()]\n",
        "best_thr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnintolvWsNt"
      },
      "source": [
        "confusion_matrix(y_test, y_score > best_thr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_xyZUXlUvPB"
      },
      "source": [
        "Inference on CV1 - ROC_AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv_U_VmtU2nY"
      },
      "source": [
        "Test Acc_CV1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kS1FqkjDlcg"
      },
      "source": [
        "n_classes = 1\n",
        "y_test = df1[\"label_y\"].values\n",
        "y_score = df1[\"label_x\"].values\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "#Plot of a ROC curve for a specific class\n",
        "roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gPQVWieU0Hu"
      },
      "source": [
        "acc = []\n",
        "for _t in thr:\n",
        "    acc.append(accuracy_score(y_test, y_score > _t))\n",
        "\n",
        "acc = np.asarray(acc)\n",
        "acc[acc.argmax()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqusOFyAU7yp"
      },
      "source": [
        "Best_thr_CV1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQR4xd22U56y"
      },
      "source": [
        "best_thr = thr[acc.argmax()]\n",
        "best_thr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9119ZFY0Wv7D"
      },
      "source": [
        "confusion_matrix(y_test, y_score > best_thr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWQcGQWcVJtv"
      },
      "source": [
        "Inference on CV2 - ROC_AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpvSgpPlVU7u"
      },
      "source": [
        "Test_ACC - CV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIVBFgCzDywy"
      },
      "source": [
        "n_classes = 1\n",
        "y_test = df2[\"label_y\"].values\n",
        "y_score = df2[\"label_x\"].values\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr, tpr, thr = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "#Plot of a ROC curve for a specific class\n",
        "roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjyt6_HVOkO"
      },
      "source": [
        "acc = []\n",
        "for _t in thr:\n",
        "    acc.append(accuracy_score(y_test, y_score > _t))\n",
        "\n",
        "acc = np.asarray(acc)\n",
        "acc[acc.argmax()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hdizq2YVe5a"
      },
      "source": [
        "Best_thr_CV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s26nZUOVVhIg"
      },
      "source": [
        "best_thr = thr[acc.argmax()]\n",
        "best_thr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inka128KW1jB"
      },
      "source": [
        "confusion_matrix(y_test, y_score > best_thr)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}